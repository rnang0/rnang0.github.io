<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大数据Hadoop | rnang0 Blog</title><meta name="description" content="HadoopHadoop是一个生态圈，分布式的基础架构，主要解决海量数据的存储和分析计算问题。  Hadoop的优势  高可靠性: Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。  高扩展性:在集群间分配任务数据，可方便的扩展数以干计的节点。  高效性:在MapReduce的思想下, Hadoop是并行工作的，以加快任务处理的速度。  高容"><meta name="keywords" content="Hadoop,HDFS,MapReduce,HBase"><meta name="author" content="rnang0"><meta name="copyright" content="rnang0"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="dns-prefetch" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="dns-prefetch" href="https://fonts.googleapis.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="dns-prefetch" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="大数据Hadoop"><meta name="twitter:description" content="HadoopHadoop是一个生态圈，分布式的基础架构，主要解决海量数据的存储和分析计算问题。  Hadoop的优势  高可靠性: Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。  高扩展性:在集群间分配任务数据，可方便的扩展数以干计的节点。  高效性:在MapReduce的思想下, Hadoop是并行工作的，以加快任务处理的速度。  高容"><meta name="twitter:image" content="https://gitee.com/rnang0/blogimage/raw/master/20210508224446.jpg"><meta property="og:type" content="article"><meta property="og:title" content="大数据Hadoop"><meta property="og:url" content="http://rnang0.github.io/2021/05/08/Hadoop/"><meta property="og:site_name" content="rnang0 Blog"><meta property="og:description" content="HadoopHadoop是一个生态圈，分布式的基础架构，主要解决海量数据的存储和分析计算问题。  Hadoop的优势  高可靠性: Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。  高扩展性:在集群间分配任务数据，可方便的扩展数以干计的节点。  高效性:在MapReduce的思想下, Hadoop是并行工作的，以加快任务处理的速度。  高容"><meta property="og:image" content="https://gitee.com/rnang0/blogimage/raw/master/20210508224446.jpg"><meta property="article:published_time" content="2021-05-07T16:00:00.000Z"><meta property="article:modified_time" content="2021-05-26T07:40:56.315Z"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = 'false'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="canonical" href="http://rnang0.github.io/2021/05/08/Hadoop/"><link rel="next" title="Go的Web框架" href="http://rnang0.github.io/2021/04/29/Go%E7%9A%84Web%E6%A1%86%E6%9E%B6/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'none',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/autor.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">67</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">93</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">9</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-mars"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop"><span class="toc-text">Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Hadoop的组成"><span class="toc-text">1. Hadoop的组成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Hadoop1与Hadoop2的区别"><span class="toc-text">1.1 Hadoop1与Hadoop2的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-HDFS架构"><span class="toc-text">1.2 HDFS架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Yarn架构"><span class="toc-text">1.3 Yarn架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-MapReduce架构"><span class="toc-text">1.4 MapReduce架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Hadoop目录结构"><span class="toc-text">2. Hadoop目录结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Hadoop运行模式"><span class="toc-text">3. Hadoop运行模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-本地运行模式"><span class="toc-text">3.1 本地运行模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-伪分布式运行模式"><span class="toc-text">3.2 伪分布式运行模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-完全分布式运行模式"><span class="toc-text">3.3 完全分布式运行模式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS"><span class="toc-text">HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-HDFS文件块大小"><span class="toc-text">1. HDFS文件块大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Shell操作"><span class="toc-text">2. Shell操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-IDEA客户端操作"><span class="toc-text">3. IDEA客户端操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-API文件操作"><span class="toc-text">3.1 API文件操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-I-O流操作"><span class="toc-text">3.2 I&#x2F;O流操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-NN和SNN"><span class="toc-text">4. NN和SNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-NN和SNN的工作机制"><span class="toc-text">4.1 NN和SNN的工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Fsimage和Edits解析"><span class="toc-text">4.2 Fsimage和Edits解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-CheckPoint时间设置"><span class="toc-text">4.3 CheckPoint时间设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-NameNode故障处理"><span class="toc-text">4.4 NameNode故障处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-NameNode高可用配置"><span class="toc-text">4.6 NameNode高可用配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-DataNode"><span class="toc-text">5. DataNode</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce"><span class="toc-text">MapReduce</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://gitee.com/rnang0/blogimage/raw/master/20210508224446.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">rnang0 Blog</a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-mars"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">大数据Hadoop</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2021-05-08 00:00:00"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2021-05-08</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2021-05-26 15:40:56"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2021-05-26</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><p>Hadoop是一个生态圈，分布式的基础架构，主要<strong>解决海量数据的存储和分析计算问题。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514095835.png" alt="1"></p>
<p>Hadoop的优势</p>
<ul>
<li><p>高可靠性: Hadoop底层<strong>维护多个数据副本，所以即使Hadoop某个计算元素</strong><br><strong>或存储出现故障，也不会导致数据的丢失。</strong></p>
</li>
<li><p>高扩展性:在集群间分配任务数据，可方便的扩展数以干计的节点。</p>
</li>
<li><p>高效性:在MapReduce的思想下, <strong>Hadoop是并行工作的</strong>，以加快任务处理的速度。</p>
</li>
<li><p>高容错性:能够自动将失败的任务重新分配。</p>
</li>
</ul>
<h2 id="1-Hadoop的组成"><a href="#1-Hadoop的组成" class="headerlink" title="1. Hadoop的组成"></a>1. Hadoop的组成</h2><h3 id="1-1-Hadoop1与Hadoop2的区别"><a href="#1-1-Hadoop1与Hadoop2的区别" class="headerlink" title="1.1 Hadoop1与Hadoop2的区别"></a>1.1 <strong>Hadoop1与Hadoop2的区别</strong></h3><p><strong>Hadoop2增加了yarn来实现资源的调度，cpu的调度和内存、磁盘的管理，从而使得MapReduce“专心”地进行计算任务。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514095236.png" alt="img"></p>
<h3 id="1-2-HDFS架构"><a href="#1-2-HDFS架构" class="headerlink" title="1.2 HDFS架构"></a>1.2 <strong>HDFS架构</strong></h3><p>三部分组成：<strong>目录 + 数据 + 目录备份</strong></p>
<ul>
<li>NameNode：存储的是<strong>元数据</strong>、<strong>文件目录名和结构</strong>、属性，<strong>每个分布式存储文件的块列表和块所在的DataNode等。</strong></li>
<li>DataNode：真正存储数据的地方，<strong>存储文件块数据，以及块数据的校验和。</strong></li>
<li>Secondary NameNode：监控HDFS状态，<strong>定时每隔断时间获取HDFS元数据的快照。</strong></li>
</ul>
<h3 id="1-3-Yarn架构"><a href="#1-3-Yarn架构" class="headerlink" title="1.3 Yarn架构"></a>1.3 <strong>Yarn架构</strong></h3><p>作为调度的功能组件，ResourceManager主要是<strong>接受请求，分发调度给后续的NodeManager</strong>。ApplicationMaster的作用是负责数据的切分，<strong>管理单个任务，为程序申请资源分配给内部任务。</strong></p>
<p><strong>引用尚硅谷的图片：</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514100445.png" alt="img"></p>
<h3 id="1-4-MapReduce架构"><a href="#1-4-MapReduce架构" class="headerlink" title="1.4 MapReduce架构"></a>1.4 <strong>MapReduce架构</strong></h3><p>MapReduce将计算过程分为两个阶段：Map和Reduce，<strong>接受过程是个并行的过程。</strong></p>
<ul>
<li>Map阶段并行处理输入数据</li>
<li>Reduce阶段对Map结果进行汇总</li>
</ul>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514101632.png" alt="img"></p>
<h2 id="2-Hadoop目录结构"><a href="#2-Hadoop目录结构" class="headerlink" title="2. Hadoop目录结构"></a>2. Hadoop目录结构</h2><p>重要目录</p>
<p>（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行<strong>操作的脚本</strong></p>
<p>（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</p>
<p>（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</p>
<p>（4）sbin目录：<strong>存放启动或停止Hadoop相关服务的脚本</strong></p>
<p><strong>例如 sbin/hdfs.sh sbin/yarn.sh</strong></p>
<p>（5）share目录：存放<strong>Hadoop的依赖jar包</strong>、文档、和官方案例</p>
<h2 id="3-Hadoop运行模式"><a href="#3-Hadoop运行模式" class="headerlink" title="3. Hadoop运行模式"></a>3. Hadoop运行模式</h2><h3 id="3-1-本地运行模式"><a href="#3-1-本地运行模式" class="headerlink" title="3.1 本地运行模式"></a>3.1 本地运行模式</h3><p>官方为我们提供了几个案例，都在share里面 <code>share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar</code></p>
<p>例如Grep和WordCount案例，使用里面 <strong>hadoop-mapreduce-examples-2.7.2.jar</strong> 进行wordcount任务计算，计算wcinput的文件中单词个数，输出到输出文件wcoutput中</p>
<p><code>yangyifan@hadoop101 hadoop-2.7.2$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput</code></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514102711.png" alt="2 (1)"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514102713.png" alt="3"></p>
<h3 id="3-2-伪分布式运行模式"><a href="#3-2-伪分布式运行模式" class="headerlink" title="3.2 伪分布式运行模式"></a>3.2 伪分布式运行模式</h3><p>在单台机器中，进行伪分布式的<strong>集群操作。</strong></p>
<p>1）配置集群的配置文件，<strong>HDFS与Yarn，core-site.xml 与 hdfs-site.xml、yarn-site.xml</strong></p>
<p>2）在<strong>单台机子上启动NameNode、DataNode、才能再启动ResourceManager与NodeManager</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103418.png" alt="img"></p>
<p>3）利用MapReduce执行WordCount案例</p>
<p><strong>查看MapReduce程序进程</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103434.png" alt="img"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103514.png" alt="img"></p>
<h3 id="3-3-完全分布式运行模式"><a href="#3-3-完全分布式运行模式" class="headerlink" title="3.3 完全分布式运行模式"></a>3.3 完全分布式运行模式</h3><p>我将启动两台机器进行完全分布式的运行模式，两台机器要关闭防火墙和设置一些ip和主机信息。</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103825.png" alt="1 (1)"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103838.png" alt="2 (1)"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103845.png" alt="3"></p>
<p><strong>由于jdk与hadoop等文件与配置挨个分发过于麻烦，所以采用编写集群分发脚本xsync来进行集群的分发。</strong></p>
<p>下面是脚本代码，#5 循环 是指分发到hadoop102和hadoop103处（两台分布式机器上）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=103; host&lt;104; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>集群配置</strong></li>
</ul>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514104239.png" alt="4"></p>
<ul>
<li><strong>集群启动</strong></li>
</ul>
<p>先要配置slaves</p>
<p><code>yangyifan@hadoop102 hadoop$ vim slaves</code></p>
<p>在该文件中增加如下内容：hadoop102 和 hadoop103，文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p>
<p><strong>以hdfs集群群启命令为sbin/start-dfs.sh，不再是sbin/hadoop-daemon.sh</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514104609.png" alt="5"></p>
<p><strong>yarn集群要在ResourceManager处启动，例如我这台就是hadoop103机器</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514104713.png" alt="6"></p>
<h1 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h1><h2 id="1-HDFS文件块大小"><a href="#1-HDFS文件块大小" class="headerlink" title="1. HDFS文件块大小"></a>1. HDFS文件块大小</h2><p>HDFS的文件在物理上是分块存储，<strong>每个块大小可以通过配置参数来进行默认是128m</strong></p>
<p><strong>总结: HDFS块的大小设置主要取决于磁盘传输速率。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514105514.png" alt="img"></p>
<p><strong>大文件进行分块存储：</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514105636.png" alt="img"></p>
<p><strong>面试题：为什么块的大小不能设置太小，也不能设置太大?</strong><br>(1) HDFS的块设置太小，<strong>会增加寻址时间</strong>，程序<strong>一直在找块的开始位置</strong>;<br>(2)如果块设置的太大，从<strong>磁盘传输数据的时间</strong>会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>
<h2 id="2-Shell操作"><a href="#2-Shell操作" class="headerlink" title="2. Shell操作"></a>2. Shell操作</h2><p>语法：<code>bin/hadoop fs</code>或者是<code>bin/hdfs dfs</code> + 具体命令，<strong>dfs是fs的实现类。</strong></p>
<p>例如：<code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /</code></p>
<p><strong>重要的操作</strong>：<code>-setrep</code>：设置HDFS中文件的副本数量 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /user/yangyifan/yangyifan.txt</code> 这里设置的副本数<strong>只是记录在NameNode的元数据中</strong>，真实的副本数量，还得看DataNode的数量。因为目前只有2台设备，最多也就2个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
<ul>
<li>一些特殊的操作</li>
</ul>
<p><code>-moveFromLocal</code>：从本地剪切粘贴到HDFS <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./yyf.txt /user/yangyifan</code></p>
<p><code>-copyFromLocal</code>：<code>从本地文件系统中拷贝文件到HDFS路径去 [yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /</code></p>
<p><code>-copyToLocal</code>：从HDFS拷贝到本地 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /user/yangyifan/yangyifan.txt ./</code></p>
<p><code>-cp</code>：从HDFS的一个路径拷贝到HDFS的另一个路径 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /user/yangyifan/yangyifan.txt /yiweicheng.txt</code></p>
<p><code>-mv</code>：在HDFS目录中移动文件 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /user/yangyifan/</code></p>
<p><code>-get</code>：等同于copyToLocal，就是从HDFS下载文件到本地 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -get /user/yangyifan/yangyifan.txt ./</code></p>
<p><code>-getmerge</code>：合并下载多个文件，比如HDFS的目录 /user/yangyifan/test下有多个文件:log.1, log.2,log.3,…  <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/yangyifan/test/* ./yihao.txt</code></p>
<p><code>-put</code>：等同于copyFromLocal <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./yihao.txt /user/yangyifan/test/</code></p>
<h2 id="3-IDEA客户端操作"><a href="#3-IDEA客户端操作" class="headerlink" title="3. IDEA客户端操作"></a>3. IDEA客户端操作</h2><p>先将<code>hadoop-2.7.2</code>下载到windows路径下，配置<strong>HADOOP_HOME环境变量，和Path环境变量。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210521130449.png" alt="image-20210521130441804"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210521130455.png" alt="image-20210521130455547"></p>
<p><strong>导入hadoop的Maven依赖</strong>，并添加日志配置文件，在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210521130805.png" alt="img"></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<h3 id="3-1-API文件操作"><a href="#3-1-API文件操作" class="headerlink" title="3.1 API文件操作"></a>3.1 API文件操作</h3><p><strong>（1）新建目录</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1 获取文件系统</span></span><br><span class="line">	Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">	<span class="comment">// 配置在集群上运行</span></span><br><span class="line">	<span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span></span><br><span class="line">	<span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line">	FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"yangyifan"</span>);</span><br><span class="line">	<span class="comment">// 2 创建目录</span></span><br><span class="line">	fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/1108/daxian/banzhang"</span>));</span><br><span class="line">	<span class="comment">// 3 关闭资源</span></span><br><span class="line">	fs.close();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>（2）文件上传</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// API测试</span></span><br><span class="line"><span class="comment">// (1)文件上传</span></span><br><span class="line">fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"g:/data.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br></pre></td></tr></table></figure>

<p><strong>（3）文件下载</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 2 执行下载操作</span></span><br><span class="line"><span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line"><span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line"><span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line"><span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/banzhang.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"g:/banhua.txt"</span>), <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>

<p><strong>（4）文件删除</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">fs.delete(<span class="keyword">new</span> Path(<span class="string">"/0508/"</span>), <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>

<p>（5）文件名更改</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">fs.rename(<span class="keyword">new</span> Path(<span class="string">"/banzhang.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/banhua.txt"</span>));</span><br></pre></td></tr></table></figure>

<p><strong>（6）查看文件名称、权限、长度、块信息</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">    LocatedFileStatus status = listFiles.next();</span><br><span class="line">    <span class="comment">// 文件名称</span></span><br><span class="line">    System.out.println(status.getPath().getName());</span><br><span class="line">    <span class="comment">// 长度</span></span><br><span class="line">    System.out.println(status.getLen());</span><br><span class="line">    <span class="comment">// 权限</span></span><br><span class="line">    System.out.println(status.getPermission());</span><br><span class="line">    <span class="comment">// 分组</span></span><br><span class="line">    System.out.println(status.getGroup());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取存储的块信息</span></span><br><span class="line">    BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line">    <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">        <span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">        String[] hosts = blockLocation.getHosts();</span><br><span class="line">        <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">            System.out.println(host);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">"---------------------"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（7）文件和文件夹判断</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));	</span><br><span class="line"><span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line">    <span class="comment">// 如果是文件</span></span><br><span class="line">    <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">        System.out.println(<span class="string">"f:"</span>+fileStatus.getPath().getName());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">"d:"</span>+fileStatus.getPath().getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-I-O流操作"><a href="#3-2-I-O流操作" class="headerlink" title="3.2 I/O流操作"></a>3.2 I/O流操作</h3><p>上面我们学的<strong>API操作HDFS系统都是框架封装好的</strong>。那么如果我们想自己实现上述API的操作该怎么实现呢？</p>
<p>我们<strong>可以采用IO流的方式实现数据的上传和下载。</strong>具体不再赘述，可以网上查看文档。</p>
<h2 id="4-NN和SNN"><a href="#4-NN和SNN" class="headerlink" title="4. NN和SNN"></a>4. NN和SNN</h2><h3 id="4-1-NN和SNN的工作机制"><a href="#4-1-NN和SNN的工作机制" class="headerlink" title="4.1 NN和SNN的工作机制"></a>4.1 NN和SNN的工作机制</h3><p><strong>NN中的元数据需要存放在内存中来保证效率，</strong>但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。</p>
<p><strong>因此磁盘中有一个备份元数据的FsImage。</strong>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。<strong>因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</strong></p>
<p>需要定期进行FsImage和Edits的合并，<strong>引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</strong></p>
<p>NN和2NN工作机制，引用尚硅谷的图：</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210526095502.png" alt="1"></p>
<p><strong>NameNode：第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</strong></p>
<p><strong>SecondaryNameNode：询问NameNode是否需要CheckPoint（定时时间到、Edit中的数据满了）请求执行CheckPoint。</strong></p>
<p><strong>然后将编辑日志和镜像文件拷贝到Secondary NameNode。SNN将其加载到内存，并合并。生成新的镜像文件fsimage.chkpoint，拷贝到NameNode。NameNode将其重新命名成fsimage。</strong></p>
<p>拓展：工作机制详解</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</span><br><span class="line"></span><br><span class="line">由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</span><br><span class="line"></span><br><span class="line">SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</span><br></pre></td></tr></table></figure>

<h3 id="4-2-Fsimage和Edits解析"><a href="#4-2-Fsimage和Edits解析" class="headerlink" title="4.2 Fsimage和Edits解析"></a>4.2 Fsimage和Edits解析</h3><p>NameNode被格式化之后，将在hadoop下<code>data/tmp/dfs/name/current</code>目录中产生如下文件<br><img src="https://gitee.com/rnang0/blogimage/raw/master/20210526101601.png" alt="img"><br>(1) Fsimage文件: HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目<br>录和文件inode的序列化信息。<br>(2) Edits文件: 存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先<br>会被记录到Edits文件中。<br>(3) <strong>seen_ txid文件保存的是一个数字， 就是最后一个edits_的数字，是当前最新的编辑日志。</strong><br>(4)每次NameNode启动的时候都会将Fsimage文件读入内存 ，加载Edits里面的更新操作，保证内存<br>中的元数据信息是最新的、同步的。</p>
<p><strong>注意：NameNode如何确定下次开机启动的时候合并哪些Edits？根据seen_txtid来获取最新的edit编辑文件进行合并</strong></p>
<p>查看文件：oiv查看Fsimage文件和oev查看Edits文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</span><br><span class="line">hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</span><br></pre></td></tr></table></figure>

<p>例如：<code>hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</code>将显示的xml文件内容拷贝到IDEA中创建的xml文件中</p>
<h3 id="4-3-CheckPoint时间设置"><a href="#4-3-CheckPoint时间设置" class="headerlink" title="4.3 CheckPoint时间设置"></a>4.3 CheckPoint时间设置</h3><p>（1）通常情况下，SecondaryNameNode<strong>定时每隔一小时</strong>执行一次。</p>
<p>在hdfs-default.xml中：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）一分钟检查一次操作次数，<strong>当操作次数达到1百万时</strong>，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="4-4-NameNode故障处理"><a href="#4-4-NameNode故障处理" class="headerlink" title="4.4 NameNode故障处理"></a>4.4 NameNode故障处理</h3><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</p>
<ol>
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</code></p>
<ol start="3">
<li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li>
</ol>
<p><code>[yangyifan@hadoop101 dfs]$ scp -r yangyifan@hadoop103:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</code></p>
<ol start="4">
<li>重新启动NameNode</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</code></p>
<p><strong>方法二：使用-importCheckpoint 选项启动 NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</strong></p>
<ol>
<li>修改<code>hdfs-site.xml</code>中的</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</code></p>
<ol start="4">
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要<strong>将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录</strong>，<strong>并删除in_use.lock文件</strong></li>
</ol>
<p><code>[yangyifan@hadoop101 dfs]$ scp -r yangyifan@hadoop103:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</code></p>
<p><code>[yangyifan@hadoop101 namesecondary]$ rm -rf in_use.lock</code></p>
<p><code>[yangyifan@hadoop101 dfs]$ pwd</code></p>
<p><code>/opt/module/hadoop-2.7.2/data/tmp/dfs</code></p>
<p><code>[yangyifan@hadoop101 dfs]$ ls</code></p>
<p><code>data name namesecondary</code></p>
<ol start="5">
<li>导入检查点数据（等待一会ctrl+c结束掉）</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</code></p>
<ol start="6">
<li>启动NameNode</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</code></p>
<h3 id="4-6-NameNode高可用配置"><a href="#4-6-NameNode高可用配置" class="headerlink" title="4.6 NameNode高可用配置"></a>4.6 NameNode高可用配置</h3><p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。</p>
<p><strong>当进行增删改查操作时，其中的镜像文件和编辑日志都一样。</strong></p>
<p>具体配置如下</p>
<p>​    （1）在hdfs-site.xml文件中增加如下内容</p>
<p><strong>注意：先删除配置文件中先前配置的 -importCheckpoint 配置</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）停止集群，删除data和logs中所有数据。</p>
<p><code>[yangyifan@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/</code></p>
<p><code>[yangyifan@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/</code></p>
<p><code>[yangyifan@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/</code></p>
<p>（3）格式化集群并启动。</p>
<p><code>[yangyifan@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format</code></p>
<p><code>[yangyifan@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</code></p>
<p>（4）查看结果</p>
<p><code>[yangyifan@hadoop102 dfs]$ ll</code></p>
<p>总用量 12</p>
<p>drwx——. 3 yangyifan yangyifan 4096 12月 11 08:03 data</p>
<p>drwxrwxr-x. 3 yangyifan yangyifan 4096 12月 11 08:03 name1</p>
<p>drwxrwxr-x. 3 yangyifan yangyifan 4096 12月 11 08:03 name2</p>
<h2 id="5-DataNode"><a href="#5-DataNode" class="headerlink" title="5. DataNode"></a>5. DataNode</h2><h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">rnang0</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://rnang0.github.io/2021/05/08/Hadoop/">http://rnang0.github.io/2021/05/08/Hadoop/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://rnang0.github.io" target="_blank">rnang0 Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/HDFS/">HDFS</a><a class="post-meta__tags" href="/tags/MapReduce/">MapReduce</a><a class="post-meta__tags" href="/tags/HBase/">HBase</a></div><div class="post_share"><div class="social-share" data-image="https://gitee.com/rnang0/blogimage/raw/master/20210508224446.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/img/wechat.jpg" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2021/04/29/Go%E7%9A%84Web%E6%A1%86%E6%9E%B6/"><img class="next_cover" src="https://avatars.githubusercontent.com/u/7894478?v=3" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Go的Web框架</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By rnang0</div><div class="footer_custom_text">Hi, welcome to my <a href="http://rnang0.github.io/">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script></body></html>