<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大数据Hadoop | rnang0 Blog</title><meta name="description" content="HadoopHadoop是一个生态圈，分布式的基础架构，主要解决海量数据的存储和分析计算问题。  Hadoop的优势  高可靠性: Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。  高扩展性:在集群间分配任务数据，可方便的扩展数以干计的节点。  高效性:在MapReduce的思想下, Hadoop是并行工作的，以加快任务处理的速度。  高容"><meta name="keywords" content="Hadoop,HDFS,MapReduce"><meta name="author" content="rnang0"><meta name="copyright" content="rnang0"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="dns-prefetch" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="dns-prefetch" href="https://fonts.googleapis.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="dns-prefetch" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="大数据Hadoop"><meta name="twitter:description" content="HadoopHadoop是一个生态圈，分布式的基础架构，主要解决海量数据的存储和分析计算问题。  Hadoop的优势  高可靠性: Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。  高扩展性:在集群间分配任务数据，可方便的扩展数以干计的节点。  高效性:在MapReduce的思想下, Hadoop是并行工作的，以加快任务处理的速度。  高容"><meta name="twitter:image" content="https://gitee.com/rnang0/blogimage/raw/master/20210508224446.jpg"><meta property="og:type" content="article"><meta property="og:title" content="大数据Hadoop"><meta property="og:url" content="http://rnang0.github.io/2021/05/08/Hadoop/"><meta property="og:site_name" content="rnang0 Blog"><meta property="og:description" content="HadoopHadoop是一个生态圈，分布式的基础架构，主要解决海量数据的存储和分析计算问题。  Hadoop的优势  高可靠性: Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。  高扩展性:在集群间分配任务数据，可方便的扩展数以干计的节点。  高效性:在MapReduce的思想下, Hadoop是并行工作的，以加快任务处理的速度。  高容"><meta property="og:image" content="https://gitee.com/rnang0/blogimage/raw/master/20210508224446.jpg"><meta property="article:published_time" content="2021-05-07T16:00:00.000Z"><meta property="article:modified_time" content="2021-06-25T08:28:36.583Z"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = 'false'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="canonical" href="http://rnang0.github.io/2021/05/08/Hadoop/"><link rel="prev" title="HBase数据库、Hive数据仓库" href="http://rnang0.github.io/2021/06/09/HBase%E5%92%8CHive/"><link rel="next" title="Go的Web框架" href="http://rnang0.github.io/2021/04/29/Go%E7%9A%84Web%E6%A1%86%E6%9E%B6/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: false,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'none',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/autor.JPG" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">68</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">94</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">9</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-mars"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop"><span class="toc-text">Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Hadoop的组成"><span class="toc-text">1. Hadoop的组成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Hadoop1与Hadoop2的区别"><span class="toc-text">1.1 Hadoop1与Hadoop2的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-HDFS架构"><span class="toc-text">1.2 HDFS架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Yarn架构"><span class="toc-text">1.3 Yarn架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-MapReduce架构"><span class="toc-text">1.4 MapReduce架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Hadoop目录结构"><span class="toc-text">2. Hadoop目录结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Hadoop运行模式"><span class="toc-text">3. Hadoop运行模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-本地运行模式"><span class="toc-text">3.1 本地运行模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-伪分布式运行模式"><span class="toc-text">3.2 伪分布式运行模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-完全分布式运行模式"><span class="toc-text">3.3 完全分布式运行模式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS"><span class="toc-text">HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-HDFS文件块大小"><span class="toc-text">1. HDFS文件块大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Shell操作"><span class="toc-text">2. Shell操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-IDEA客户端操作"><span class="toc-text">3. IDEA客户端操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-API文件操作"><span class="toc-text">3.1 API文件操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-I-O流操作"><span class="toc-text">3.2 I&#x2F;O流操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-NN和SNN"><span class="toc-text">4. NN和SNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-NN和SNN的工作机制"><span class="toc-text">4.1 NN和SNN的工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Fsimage和Edits解析"><span class="toc-text">4.2 Fsimage和Edits解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-CheckPoint时间设置"><span class="toc-text">4.3 CheckPoint时间设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-NameNode故障处理"><span class="toc-text">4.4 NameNode故障处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-NameNode高可用配置"><span class="toc-text">4.6 NameNode高可用配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-DataNode"><span class="toc-text">5. DataNode</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-DataNode工作机制"><span class="toc-text">5.1 DataNode工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-DataNode损坏"><span class="toc-text">5.2 DataNode损坏</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#（1）数据完整性"><span class="toc-text">（1）数据完整性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（2）掉线时限参数设置"><span class="toc-text">（2）掉线时限参数设置</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-服役新DataNode（白名单）"><span class="toc-text">5.3 服役新DataNode（白名单）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-黑名单退役"><span class="toc-text">5.4 黑名单退役</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-Datanode高可用"><span class="toc-text">5.5 Datanode高可用</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce"><span class="toc-text">MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-MapReduce工作流程"><span class="toc-text">1. MapReduce工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Mapper、Reducer和Driver编程"><span class="toc-text">2. Mapper、Reducer和Driver编程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-序列化"><span class="toc-text">3. 序列化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-InputFormat数据输入"><span class="toc-text">4. InputFormat数据输入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-FileInputFormat实现类"><span class="toc-text">5. FileInputFormat实现类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-FileInputFormat切片机制"><span class="toc-text">5.1 FileInputFormat切片机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-TextInputFormat切片实现"><span class="toc-text">5.2 TextInputFormat切片实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-CombineTextInputFormat切片实现"><span class="toc-text">5.3 CombineTextInputFormat切片实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-OutputFormat接口实现类"><span class="toc-text">6. OutputFormat接口实现类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Shuffle机制"><span class="toc-text">7. Shuffle机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Partition分区"><span class="toc-text">7.1 Partition分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-WritableComparable排序"><span class="toc-text">7.2 WritableComparable排序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Combiner合并"><span class="toc-text">8. Combiner合并</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-GroupingComparator分组排序"><span class="toc-text">9. GroupingComparator分组排序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-MapTask与ReduceTask工作机制（-补充）"><span class="toc-text">10. MapTask与ReduceTask工作机制（ 补充）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Yarn"><span class="toc-text">Yarn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Yarn基本架构"><span class="toc-text">1. Yarn基本架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Yarn工作机制（MR作业提交过程）"><span class="toc-text">2. Yarn工作机制（MR作业提交过程）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-资源调度器——FIFO、Capacity-Scheduler容量-和Fair-Scheduler公平"><span class="toc-text">3. 资源调度器——FIFO、Capacity Scheduler容量 和Fair Scheduler公平</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://gitee.com/rnang0/blogimage/raw/master/20210508224446.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">rnang0 Blog</a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-mars"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">大数据Hadoop</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2021-05-08 00:00:00"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2021-05-08</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2021-06-25 16:28:36"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2021-06-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><p>Hadoop是一个生态圈，分布式的基础架构，主要<strong>解决海量数据的存储和分析计算问题。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514095835.png" alt="1"></p>
<p>Hadoop的优势</p>
<ul>
<li><p>高可靠性: Hadoop底层<strong>维护多个数据副本，所以即使Hadoop某个计算元素</strong><br><strong>或存储出现故障，也不会导致数据的丢失。</strong></p>
</li>
<li><p>高扩展性:在集群间分配任务数据，可方便的扩展数以干计的节点。</p>
</li>
<li><p>高效性:在MapReduce的思想下, <strong>Hadoop是并行工作的</strong>，以加快任务处理的速度。</p>
</li>
<li><p>高容错性:能够自动将失败的任务重新分配。</p>
</li>
</ul>
<h2 id="1-Hadoop的组成"><a href="#1-Hadoop的组成" class="headerlink" title="1. Hadoop的组成"></a>1. Hadoop的组成</h2><h3 id="1-1-Hadoop1与Hadoop2的区别"><a href="#1-1-Hadoop1与Hadoop2的区别" class="headerlink" title="1.1 Hadoop1与Hadoop2的区别"></a>1.1 <strong>Hadoop1与Hadoop2的区别</strong></h3><p><strong>Hadoop2增加了yarn来实现资源的调度，cpu的调度和内存、磁盘的管理，从而使得MapReduce“专心”地进行计算任务。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514095236.png" alt="img"></p>
<h3 id="1-2-HDFS架构"><a href="#1-2-HDFS架构" class="headerlink" title="1.2 HDFS架构"></a>1.2 <strong>HDFS架构</strong></h3><p>三部分组成：<strong>目录 + 数据 + 目录备份</strong></p>
<ul>
<li>NameNode：存储的是<strong>元数据</strong>、<strong>文件目录名和结构</strong>、属性，<strong>每个分布式存储文件的块列表和块所在的DataNode等。</strong></li>
<li>DataNode：真正存储数据的地方，<strong>存储文件块数据，以及块数据的校验和。</strong></li>
<li>Secondary NameNode：监控HDFS状态，<strong>定时每隔断时间获取HDFS元数据的快照。</strong></li>
</ul>
<h3 id="1-3-Yarn架构"><a href="#1-3-Yarn架构" class="headerlink" title="1.3 Yarn架构"></a>1.3 <strong>Yarn架构</strong></h3><p>作为调度的功能组件，ResourceManager主要是<strong>接受请求，分发调度给后续的NodeManager</strong>。ApplicationMaster的作用是负责数据的切分，<strong>管理单个任务，为程序申请资源分配给内部任务。</strong></p>
<p><strong>引用尚硅谷的图片：</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514100445.png" alt="img"></p>
<h3 id="1-4-MapReduce架构"><a href="#1-4-MapReduce架构" class="headerlink" title="1.4 MapReduce架构"></a>1.4 <strong>MapReduce架构</strong></h3><p>MapReduce将计算过程分为两个阶段：Map和Reduce，<strong>接受过程是个并行的过程。</strong></p>
<ul>
<li>Map阶段并行处理输入数据</li>
<li>Reduce阶段对Map结果进行汇总</li>
</ul>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514101632.png" alt="img"></p>
<h2 id="2-Hadoop目录结构"><a href="#2-Hadoop目录结构" class="headerlink" title="2. Hadoop目录结构"></a>2. Hadoop目录结构</h2><p>重要目录</p>
<p>（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行<strong>操作的脚本</strong></p>
<p>（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</p>
<p>（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</p>
<p>（4）sbin目录：<strong>存放启动或停止Hadoop相关服务的脚本</strong></p>
<p><strong>例如 sbin/hdfs.sh sbin/yarn.sh</strong></p>
<p>（5）share目录：存放<strong>Hadoop的依赖jar包</strong>、文档、和官方案例</p>
<h2 id="3-Hadoop运行模式"><a href="#3-Hadoop运行模式" class="headerlink" title="3. Hadoop运行模式"></a>3. Hadoop运行模式</h2><h3 id="3-1-本地运行模式"><a href="#3-1-本地运行模式" class="headerlink" title="3.1 本地运行模式"></a>3.1 本地运行模式</h3><p>官方为我们提供了几个案例，都在share里面 <code>share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar</code></p>
<p>例如Grep和WordCount案例，使用里面 <strong>hadoop-mapreduce-examples-2.7.2.jar</strong> 进行wordcount任务计算，计算wcinput的文件中单词个数，输出到输出文件wcoutput中</p>
<p><code>yangyifan@hadoop101 hadoop-2.7.2$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput</code></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514102711.png" alt="2 (1)"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514102713.png" alt="3"></p>
<h3 id="3-2-伪分布式运行模式"><a href="#3-2-伪分布式运行模式" class="headerlink" title="3.2 伪分布式运行模式"></a>3.2 伪分布式运行模式</h3><p>在单台机器中，进行伪分布式的<strong>集群操作。</strong></p>
<p>1）配置集群的配置文件，<strong>HDFS与Yarn，core-site.xml 与 hdfs-site.xml、yarn-site.xml</strong></p>
<p>2）在<strong>单台机子上启动NameNode、DataNode、才能再启动ResourceManager与NodeManager</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103418.png" alt="img"></p>
<p>3）利用MapReduce执行WordCount案例</p>
<p><strong>查看MapReduce程序进程</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103434.png" alt="img"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103514.png" alt="img"></p>
<h3 id="3-3-完全分布式运行模式"><a href="#3-3-完全分布式运行模式" class="headerlink" title="3.3 完全分布式运行模式"></a>3.3 完全分布式运行模式</h3><p>我将启动两台机器进行完全分布式的运行模式，两台机器要关闭防火墙和设置一些ip和主机信息。</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103825.png" alt="1 (1)"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103838.png" alt="2 (1)"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514103845.png" alt="3"></p>
<p><strong>由于jdk与hadoop等文件与配置挨个分发过于麻烦，所以采用编写集群分发脚本xsync来进行集群的分发。</strong></p>
<p>下面是脚本代码，#5 循环 是指分发到hadoop102和hadoop103处（两台分布式机器上）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=103; host&lt;104; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>集群配置</strong></li>
</ul>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514104239.png" alt="4"></p>
<ul>
<li><strong>单点启动</strong></li>
</ul>
<p><code>sbin/hadoop-daemon.sh start namenode</code></p>
<p><code>sbin/yarn-daemon.sh start resourcemanager</code></p>
<ul>
<li><strong>集群启动</strong></li>
</ul>
<p>先要配置slaves</p>
<p><code>yangyifan@hadoop102 hadoop$ vim slaves</code></p>
<p>在该文件中增加如下内容：hadoop102 和 hadoop103，文件中添加的内容结尾不允许有空格，文件中不允许有空行。</p>
<p>集群群启命令为<code>sbin/start-dfs.sh``sbin/start-yarn.sh</code></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514104609.png" alt="5"></p>
<p><strong>yarn集群要在ResourceManager处启动，例如我这台就是hadoop103机器</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514104713.png" alt="6"></p>
<h1 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h1><h2 id="1-HDFS文件块大小"><a href="#1-HDFS文件块大小" class="headerlink" title="1. HDFS文件块大小"></a>1. HDFS文件块大小</h2><p>HDFS的文件在物理上是分块存储，<strong>每个块大小可以通过配置参数来进行默认是128m</strong></p>
<p><strong>总结: HDFS块的大小设置主要取决于磁盘传输速率。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514105514.png" alt="img"></p>
<p><strong>大文件进行分块存储：</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210514105636.png" alt="img"></p>
<p><strong>面试题：为什么块的大小不能设置太小，也不能设置太大?</strong><br>(1) HDFS的块设置太小，<strong>会增加寻址时间</strong>，程序<strong>一直在找块的开始位置</strong>;<br>(2)如果块设置的太大，从<strong>磁盘传输数据的时间</strong>会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>
<h2 id="2-Shell操作"><a href="#2-Shell操作" class="headerlink" title="2. Shell操作"></a>2. Shell操作</h2><p>语法：<code>bin/hadoop fs</code>或者是<code>bin/hdfs dfs</code> + 具体命令，<strong>dfs是fs的实现类。</strong></p>
<p>例如：<code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /</code></p>
<p><strong>重要的操作</strong>：<code>-setrep</code>：设置HDFS中文件的副本数量 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /user/yangyifan/yangyifan.txt</code> 这里设置的副本数<strong>只是记录在NameNode的元数据中</strong>，真实的副本数量，还得看DataNode的数量。因为目前只有2台设备，最多也就2个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
<ul>
<li>一些特殊的操作</li>
</ul>
<p><code>-moveFromLocal</code>：从本地剪切粘贴到HDFS <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./yyf.txt /user/yangyifan</code></p>
<p><code>-copyFromLocal</code>：<code>从本地文件系统中拷贝文件到HDFS路径去 [yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /</code></p>
<p><code>-copyToLocal</code>：从HDFS拷贝到本地 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /user/yangyifan/yangyifan.txt ./</code></p>
<p><code>-cp</code>：从HDFS的一个路径拷贝到HDFS的另一个路径 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /user/yangyifan/yangyifan.txt /yiweicheng.txt</code></p>
<p><code>-mv</code>：在HDFS目录中移动文件 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /user/yangyifan/</code></p>
<p><code>-get</code>：等同于copyToLocal，就是从HDFS下载文件到本地 <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -get /user/yangyifan/yangyifan.txt ./</code></p>
<p><code>-getmerge</code>：合并下载多个文件，比如HDFS的目录 /user/yangyifan/test下有多个文件:log.1, log.2,log.3,…  <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/yangyifan/test/* ./yihao.txt</code></p>
<p><code>-put</code>：等同于copyFromLocal <code>[yangyifan@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./yihao.txt /user/yangyifan/test/</code></p>
<h2 id="3-IDEA客户端操作"><a href="#3-IDEA客户端操作" class="headerlink" title="3. IDEA客户端操作"></a>3. IDEA客户端操作</h2><p>先将<code>hadoop-2.7.2</code>下载到windows路径下，配置<strong>HADOOP_HOME环境变量，和Path环境变量。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210521130449.png" alt="image-20210521130441804"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210521130455.png" alt="image-20210521130455547"></p>
<p><strong>导入hadoop的Maven依赖</strong>，并添加日志配置文件，在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210521130805.png" alt="img"></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<h3 id="3-1-API文件操作"><a href="#3-1-API文件操作" class="headerlink" title="3.1 API文件操作"></a>3.1 API文件操作</h3><p><strong>（1）新建目录</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1 获取文件系统</span></span><br><span class="line">	Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">	<span class="comment">// 配置在集群上运行</span></span><br><span class="line">	<span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span></span><br><span class="line">	<span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line">	FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"yangyifan"</span>);</span><br><span class="line">	<span class="comment">// 2 创建目录</span></span><br><span class="line">	fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/1108/daxian/banzhang"</span>));</span><br><span class="line">	<span class="comment">// 3 关闭资源</span></span><br><span class="line">	fs.close();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>（2）文件上传</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// API测试</span></span><br><span class="line"><span class="comment">// (1)文件上传</span></span><br><span class="line">fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"g:/data.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br></pre></td></tr></table></figure>

<p><strong>（3）文件下载</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 2 执行下载操作</span></span><br><span class="line"><span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line"><span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line"><span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line"><span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/banzhang.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"g:/banhua.txt"</span>), <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>

<p><strong>（4）文件删除</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">fs.delete(<span class="keyword">new</span> Path(<span class="string">"/0508/"</span>), <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>

<p>（5）文件名更改</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">fs.rename(<span class="keyword">new</span> Path(<span class="string">"/banzhang.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/banhua.txt"</span>));</span><br></pre></td></tr></table></figure>

<p><strong>（6）查看文件名称、权限、长度、块信息</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">    LocatedFileStatus status = listFiles.next();</span><br><span class="line">    <span class="comment">// 文件名称</span></span><br><span class="line">    System.out.println(status.getPath().getName());</span><br><span class="line">    <span class="comment">// 长度</span></span><br><span class="line">    System.out.println(status.getLen());</span><br><span class="line">    <span class="comment">// 权限</span></span><br><span class="line">    System.out.println(status.getPermission());</span><br><span class="line">    <span class="comment">// 分组</span></span><br><span class="line">    System.out.println(status.getGroup());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取存储的块信息</span></span><br><span class="line">    BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line">    <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">        <span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">        String[] hosts = blockLocation.getHosts();</span><br><span class="line">        <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">            System.out.println(host);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">"---------------------"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（7）文件和文件夹判断</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));	</span><br><span class="line"><span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line">    <span class="comment">// 如果是文件</span></span><br><span class="line">    <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">        System.out.println(<span class="string">"f:"</span>+fileStatus.getPath().getName());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">"d:"</span>+fileStatus.getPath().getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-I-O流操作"><a href="#3-2-I-O流操作" class="headerlink" title="3.2 I/O流操作"></a>3.2 I/O流操作</h3><p>上面我们学的<strong>API操作HDFS系统都是框架封装好的</strong>。那么如果我们想自己实现上述API的操作该怎么实现呢？</p>
<p>我们<strong>可以采用IO流的方式实现数据的上传和下载。</strong>具体不再赘述，可以网上查看文档。</p>
<h2 id="4-NN和SNN"><a href="#4-NN和SNN" class="headerlink" title="4. NN和SNN"></a>4. NN和SNN</h2><h3 id="4-1-NN和SNN的工作机制"><a href="#4-1-NN和SNN的工作机制" class="headerlink" title="4.1 NN和SNN的工作机制"></a>4.1 NN和SNN的工作机制</h3><p><strong>NN中的元数据需要存放在内存中来保证效率，</strong>但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。</p>
<p><strong>因此磁盘中有一个备份元数据的FsImage。</strong>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。<strong>因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</strong></p>
<p>需要定期进行FsImage和Edits的合并，<strong>引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</strong></p>
<p>NN和2NN工作机制，引用尚硅谷的图：</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210526095502.png" alt="1"></p>
<p><strong>NameNode：第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</strong></p>
<p><strong>SecondaryNameNode：询问NameNode是否需要CheckPoint（定时时间到、Edit中的数据满了）请求执行CheckPoint。</strong></p>
<p><strong>然后将编辑日志和镜像文件拷贝到Secondary NameNode。SNN将其加载到内存，并合并。生成新的镜像文件fsimage.chkpoint，拷贝到NameNode。NameNode将其重新命名成fsimage。</strong></p>
<p>拓展：工作机制详解</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</span><br><span class="line"></span><br><span class="line">由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</span><br><span class="line"></span><br><span class="line">SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</span><br></pre></td></tr></table></figure>

<h3 id="4-2-Fsimage和Edits解析"><a href="#4-2-Fsimage和Edits解析" class="headerlink" title="4.2 Fsimage和Edits解析"></a>4.2 Fsimage和Edits解析</h3><p>NameNode被格式化之后，将在hadoop下<code>data/tmp/dfs/name/current</code>目录中产生如下文件<br><img src="https://gitee.com/rnang0/blogimage/raw/master/20210526101601.png" alt="img"><br>(1) Fsimage文件: HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目<br>录和文件inode的序列化信息。<br>(2) Edits文件: 存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先<br>会被记录到Edits文件中。<br>(3) <strong>seen_ txid文件保存的是一个数字， 就是最后一个edits_的数字，是当前最新的编辑日志。</strong><br>(4)每次NameNode启动的时候都会将Fsimage文件读入内存 ，加载Edits里面的更新操作，保证内存<br>中的元数据信息是最新的、同步的。</p>
<p><strong>注意：NameNode如何确定下次开机启动的时候合并哪些Edits？根据seen_txtid来获取最新的edit编辑文件进行合并</strong></p>
<p>查看文件：oiv查看Fsimage文件和oev查看Edits文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</span><br><span class="line">hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</span><br></pre></td></tr></table></figure>

<p>例如：<code>hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</code>将显示的xml文件内容拷贝到IDEA中创建的xml文件中</p>
<h3 id="4-3-CheckPoint时间设置"><a href="#4-3-CheckPoint时间设置" class="headerlink" title="4.3 CheckPoint时间设置"></a>4.3 CheckPoint时间设置</h3><p>（1）通常情况下，SecondaryNameNode<strong>定时每隔一小时</strong>执行一次。</p>
<p>在hdfs-default.xml中：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）一分钟检查一次操作次数，<strong>当操作次数达到1百万时</strong>，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="4-4-NameNode故障处理"><a href="#4-4-NameNode故障处理" class="headerlink" title="4.4 NameNode故障处理"></a>4.4 NameNode故障处理</h3><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</p>
<ol>
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</code></p>
<ol start="3">
<li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li>
</ol>
<p><code>[yangyifan@hadoop101 dfs]$ scp -r yangyifan@hadoop103:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</code></p>
<ol start="4">
<li>重新启动NameNode</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</code></p>
<p><strong>方法二：使用-importCheckpoint 选项启动 NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</strong></p>
<ol>
<li>修改<code>hdfs-site.xml</code>中的</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</code></p>
<ol start="4">
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要<strong>将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录</strong>，<strong>并删除in_use.lock文件</strong></li>
</ol>
<p><code>[yangyifan@hadoop101 dfs]$ scp -r yangyifan@hadoop103:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</code></p>
<p><code>[yangyifan@hadoop101 namesecondary]$ rm -rf in_use.lock</code></p>
<p><code>[yangyifan@hadoop101 dfs]$ pwd</code></p>
<p><code>/opt/module/hadoop-2.7.2/data/tmp/dfs</code></p>
<p><code>[yangyifan@hadoop101 dfs]$ ls</code></p>
<p><code>data name namesecondary</code></p>
<ol start="5">
<li>导入检查点数据（等待一会ctrl+c结束掉）</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</code></p>
<ol start="6">
<li>启动NameNode</li>
</ol>
<p><code>[yangyifan@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</code></p>
<h3 id="4-6-NameNode高可用配置"><a href="#4-6-NameNode高可用配置" class="headerlink" title="4.6 NameNode高可用配置"></a>4.6 NameNode高可用配置</h3><p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。</p>
<p><strong>当进行增删改查操作时，其中的镜像文件和编辑日志都一样。</strong></p>
<p>具体配置如下</p>
<p>​    （1）在hdfs-site.xml文件中增加如下内容</p>
<p><strong>注意：先删除配置文件中先前配置的 -importCheckpoint 配置</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）停止集群，删除data和logs中所有数据。</p>
<p><code>[yangyifan@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/</code></p>
<p><code>[yangyifan@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/</code></p>
<p><code>[yangyifan@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/</code></p>
<p>（3）格式化集群并启动。</p>
<p><code>[yangyifan@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format</code></p>
<p><code>[yangyifan@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</code></p>
<p>（4）查看结果</p>
<p><code>[yangyifan@hadoop102 dfs]$ ll</code></p>
<p>总用量 12</p>
<p>drwx——. 3 yangyifan yangyifan 4096 12月 11 08:03 data</p>
<p>drwxrwxr-x. 3 yangyifan yangyifan 4096 12月 11 08:03 name1</p>
<p>drwxrwxr-x. 3 yangyifan yangyifan 4096 12月 11 08:03 name2</p>
<h2 id="5-DataNode"><a href="#5-DataNode" class="headerlink" title="5. DataNode"></a>5. DataNode</h2><h3 id="5-1-DataNode工作机制"><a href="#5-1-DataNode工作机制" class="headerlink" title="5.1 DataNode工作机制"></a>5.1 DataNode工作机制</h3><p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210531191544.png" alt="image-20210531191537568"></p>
<p>1）一个<strong>数据块Block</strong>在DataNode上<strong>以文件形式</strong>存储在磁盘上，包括两个文件，一个是数据本身，<strong>一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</strong></p>
<p>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</p>
<p>3）<strong>心跳是每3秒一次</strong>，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。<strong>如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</strong></p>
<p>4）集群运行中可以安全加入和退出一些机器。</p>
<p><strong>注意：最好提前设置白名单。</strong></p>
<h3 id="5-2-DataNode损坏"><a href="#5-2-DataNode损坏" class="headerlink" title="5.2 DataNode损坏"></a>5.2 DataNode损坏</h3><h4 id="（1）数据完整性"><a href="#（1）数据完整性" class="headerlink" title="（1）数据完整性"></a>（1）数据完整性</h4><p>DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？</p>
<p>如下是DataNode节点保证数据完整性的方法。</p>
<p>1）当DataNode读取Block的时候，它会计算CheckSum校验和。</p>
<p>2）<strong>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</strong></p>
<p>3）Client读取其他DataNode上的Block。</p>
<p>4）DataNode在其文件创建后周期验证CheckSum</p>
<h4 id="（2）掉线时限参数设置"><a href="#（2）掉线时限参数设置" class="headerlink" title="（2）掉线时限参数设置"></a>（2）掉线时限参数设置</h4><p>DataNode进程掉线或者故障，<strong>超过超时时间判断为死亡</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210531192259.png" alt="image-20210531192259406"></p>
<p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="5-3-服役新DataNode（白名单）"><a href="#5-3-服役新DataNode（白名单）" class="headerlink" title="5.3 服役新DataNode（白名单）"></a>5.3 服役新DataNode（白名单）</h3><p>我们需要在<strong>原有集群基础上动态添加新的数据节点。</strong></p>
<ol>
<li>环境准备</li>
</ol>
<p>​    （1）在hadoop103主机上再克隆一台hadoop104主机</p>
<p>​    （2）修改IP地址和主机名称</p>
<p>​    （3）<strong>删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）</strong></p>
<p>​    （4）source一下配置文件 <code>[yangyifan@hadoop104 hadoop-2.7.2]$ source /etc/profile</code></p>
<ol start="2">
<li>添加到<strong>白名单</strong></li>
</ol>
<p>（1）在<strong>NameNode</strong>的/opt/module/hadoop-2.7.2/etc/hadoop目录下<strong>创建dfs.hosts文件</strong></p>
<p>添加如下hadoop104</p>
<p>（2）在NameNode的<strong>hdfs-site.xml</strong>配置文件中<strong>增加dfs.hosts属性，开启白名单</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（3）配置文件分发</p>
<p><code>xsync hdfs-site.xml</code></p>
<p><strong>（4）刷新NameNode</strong></p>
<p><code>hdfs dfsadmin -refreshNodes</code></p>
<p>（5）<strong>更新ResourceManager节点</strong></p>
<p><code>yarn rmadmin -refreshNodes</code></p>
<p>注意：如果数据不均衡，可以用命令实现集群的再平衡</p>
<p><code>[yangyifan@hadoop101 sbin]$ ./start-balancer.sh</code></p>
<ol start="3">
<li><strong>直接启动DataNode，即可关联到集群</strong></li>
</ol>
<p><code>[yangyifan@hadoop104 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</code></p>
<p><code>[yangyifan@hadoop104 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager</code></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210531193215.png" alt="image-20210531193215264"></p>
<h3 id="5-4-黑名单退役"><a href="#5-4-黑名单退役" class="headerlink" title="5.4 黑名单退役"></a>5.4 黑名单退役</h3><p>在黑名单上面的主机都会<strong>被强制退出。</strong>如果退役的小于设置的副本数，则不允许退役。</p>
<ol>
<li>在<strong>NameNode</strong>的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建<strong>dfs.hosts.exclude文件</strong></li>
</ol>
<p>添加如下主机名称（要退役的节点）</p>
<p>hadoop103</p>
<ol start="2">
<li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>刷新NameNode、刷新ResourceManager</strong></li>
</ol>
<p><code>hdfs dfsadmin -refreshNodes</code></p>
<p><code>yarn rmadmin -refreshNodes</code></p>
<ol start="4">
<li>检查Web浏览器，退役节点的状态为<strong>decommission in progress（退役中）</strong>，说明数据节点正在复制块到其他节点，如图：                 </li>
</ol>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210531193444.png" alt="image-20210531193444561"></p>
<ol start="5">
<li><strong>等待退役节点状态为decommissioned（所有块已经复制完成）</strong>，停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役。</li>
</ol>
<p><code>sbin/hadoop-daemon.sh stop datanode</code></p>
<p><code>sbin/yarn-daemon.sh stop nodemanager</code></p>
<ol start="6">
<li>如果数据不均衡，可以用命令实现集群的再平衡</li>
</ol>
<p>sbin/start-balancer.sh </p>
<p>注意：不允许白名单和黑名单中同时出现同一个主机名称。</p>
<h3 id="5-5-Datanode高可用"><a href="#5-5-Datanode高可用" class="headerlink" title="5.5 Datanode高可用"></a>5.5 Datanode高可用</h3><ol>
<li><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</p>
</li>
<li><p>具体配置如下 <strong>hdfs-site.xml</strong></p>
</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><p>MapReduce是一个<strong>分布式运算程序</strong>的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个<strong>完整的分布式运算程序</strong>，并发运行在一个Hadoop集群上。</p>
<h2 id="1-MapReduce工作流程"><a href="#1-MapReduce工作流程" class="headerlink" title="1. MapReduce工作流程"></a>1. MapReduce工作流程</h2><p>一个完整的MapReduce程序在分布式运行时有三类实例进程:</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615185746.png" alt="1"></p>
<p>1) MrAppMaster：负责整个程序的过程调度及状态协调。</p>
<p><strong>2) MapTask：负责Map阶段的整个数据处理流程。</strong></p>
<p><strong>3) ReduceTask：负责Reduce阶段的整个数据处理流程。</strong></p>
<p>下面的流程是整个MapReduce最全工作流程，但是<strong>Shuffle过程只是从第7步开始到第16步结束</strong>，具体Shuffle过程详解，如下：</p>
<p>1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</p>
<p>2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p>
<p>3）多个溢出文件会被合并成大的溢出文件</p>
<p>4）在<strong>溢出过程及合并</strong>的过程中，都要调用Partitioner进行<strong>分区和针对key进行排序</strong></p>
<p>5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</p>
<p>6）ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</p>
<p>7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615201931.png" alt="6"></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615201920.png" alt="7"></p>
<h2 id="2-Mapper、Reducer和Driver编程"><a href="#2-Mapper、Reducer和Driver编程" class="headerlink" title="2. Mapper、Reducer和Driver编程"></a>2. Mapper、Reducer和Driver编程</h2><p>用户编写的程序分成三个部分：Mapper、Reducer和Driver。</p>
<ul>
<li><p>Mapper阶段</p>
<p>(1)用户自定义的Mapper要继承自己的父类<br>(2) Mapper的输入数据是KV对的形式(KV的类型可自定义)<br>(3) Mapper中的业务逻辑写在map()方法中<br>(4) Mapper的输出数据是KV对的形式(KV的类型可自定义)<br><strong>(5) map()方法(MapTak进程) 对每一个&lt;K,V&gt;调用一次</strong></p>
</li>
<li><p>Reducer阶段<br>(1)用户自定义的Reducer要继承自己的父类<br>(2) Reducer的输入数据类型对应Mapper的输出数据类型，也是KV<br>(3) Reducer的业务逻辑写在reduce()方法中<br><strong>(4) ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法</strong></p>
</li>
</ul>
<p><strong>如何认为相同？默认是相同对象，后续可以进行分组排序，指定按照某一个或者多个字段相同则是同一个key。</strong></p>
<ul>
<li>Driver阶段<br>相当于YARN集群的客户端，<strong>用于提交我们整个程序到YARN集群，提交的是</strong><br><strong>封装了MapReduce程序相关运行参数的job对象</strong></li>
</ul>
<h2 id="3-序列化"><a href="#3-序列化" class="headerlink" title="3. 序列化"></a>3. 序列化</h2><p>Hadoop不使用Java的序列化，因为它是一个重量级序列化框架(Serializable) ，一个对象被序列化后，会<strong>附带很多额外的信息(各种校验信息，Header, 继承体系等)，</strong> 不便于在网络中高效传输。</p>
<p><strong>Hadoop使用自己的一套序列化机制 (Writable)</strong><br>Hadoop序列化特点:（1）高效使用存储空间。（2）读写数据的额外开销小。</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615191633.png" alt="2"></p>
<p>自定义bean实现序列化，<strong>必须要实现序列化接口（Writable）</strong></p>
<p>（1）必须<strong>实现Writable接口</strong></p>
<p>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;   </span><br><span class="line">    <span class="keyword">super</span>();  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）重写序列化和反序列化方法，<strong>注意反序列化的顺序和序列化的顺序完全一致</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span>  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput  out)</span> <span class="keyword">throws</span> IOException </span>&#123;    </span><br><span class="line">    out.writeLong(upFlow);   </span><br><span class="line">    out.writeLong(downFlow);    </span><br><span class="line">    out.writeLong(sumFlow);  </span><br><span class="line">&#125;    </span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span>  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput  in)</span> <span class="keyword">throws</span> IOException </span>&#123;    </span><br><span class="line">    upFlow  = in.readLong();    </span><br><span class="line">    downFlow  = in.readLong();    </span><br><span class="line">    sumFlow  = in.readLong();  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（4）<strong>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。</strong></p>
<p>（5）如果需要将自定义的<strong>bean放在key中传输，则还需要实现Comparable接口</strong>，<strong>因为MapReduce框中的Shuffle过程要求对key必须能排序。</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span>  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span>  </span>&#123;    </span><br><span class="line">    <span class="comment">//  倒序排列，从大到小    </span></span><br><span class="line">    <span class="keyword">return</span>  <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="4-InputFormat数据输入"><a href="#4-InputFormat数据输入" class="headerlink" title="4. InputFormat数据输入"></a>4. InputFormat数据输入</h2><p><strong>数据块：</strong>Block是HDFS<strong>物理上</strong>把数据分成一块一块。</p>
<p><strong>数据切片：</strong>数据切片只是在<strong>逻辑上</strong>对输入进行分片，并不会在磁盘上将其切分成片进行存储。</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615192610.png" alt="3"></p>
<h2 id="5-FileInputFormat实现类"><a href="#5-FileInputFormat实现类" class="headerlink" title="5. FileInputFormat实现类"></a>5. FileInputFormat实现类</h2><h3 id="5-1-FileInputFormat切片机制"><a href="#5-1-FileInputFormat切片机制" class="headerlink" title="5.1 FileInputFormat切片机制"></a>5.1 FileInputFormat切片机制</h3><p><strong>切片大小默认是Block大小，不考虑数据集整体，切时只针对本文件进行切分。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615192759.png" alt="4"></p>
<p>在运行MapReduce程序时，输入的文件格式包括:基于行的日志文件、二进制格式文件、数据库表等。那么，针对不同的数据类型, MapReduce是如何读取这些数据的呢?</p>
<p>FileInputFormat常见的接口实现类包括:<strong>TextInputFormat</strong>、KeyValueTextInputFormat、NLineInputFormnat、 <strong>CombineTextInputFormat</strong>和自定义InputFormat等。</p>
<h3 id="5-2-TextInputFormat切片实现"><a href="#5-2-TextInputFormat切片实现" class="headerlink" title="5.2 TextInputFormat切片实现"></a>5.2 TextInputFormat切片实现</h3><p><strong>TextInputFormat是默认的FileInputFormat实现类。</strong></p>
<p><strong>按行读取</strong>每条记录。键是存储该行在整个文件中的，<strong>key：起始字节偏移量，LongWritable类型。</strong>值是这行的内容 <strong>value：Text类型</strong>，不包括任何行终止符( 换行符和回车符)，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例：4条文本记录。</span><br><span class="line">Rich learning form</span><br><span class="line">Inte lligent learning engine</span><br><span class="line">Lear ning more C onvenient</span><br><span class="line">From the real demand for more close to the enter prise</span><br><span class="line">每条记录表示为以下键&#x2F;值对:</span><br><span class="line">(0, Rich learning form)</span><br><span class="line">(19, Intelligent learning engine)</span><br><span class="line">(47, Le arning more convenient)</span><br><span class="line">(72 , From the real demand for more close to the ente rprise)</span><br></pre></td></tr></table></figure>

<h3 id="5-3-CombineTextInputFormat切片实现"><a href="#5-3-CombineTextInputFormat切片实现" class="headerlink" title="5.3 CombineTextInputFormat切片实现"></a>5.3 CombineTextInputFormat切片实现</h3><p>框架默认的TextInputFormat切片机制<strong>是对任务按文件规划切片</strong>，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，<strong>这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</strong>CombineTextInputFormat它可以将<strong>多个小文件从逻辑上规划到一个切片中</strong>，这样，多个小文件就可以交给一个MapTask处理。</p>
<p><strong>虚拟存储切片最大值设置（在Driver里面）</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置20m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">20971520</span>);</span><br></pre></td></tr></table></figure>

<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<ul>
<li><strong>切片机制</strong></li>
</ul>
<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>
<p><strong>当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615194410.png" alt="5"></p>
<h2 id="6-OutputFormat接口实现类"><a href="#6-OutputFormat接口实现类" class="headerlink" title="6. OutputFormat接口实现类"></a>6. OutputFormat接口实现类</h2><p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。</p>
<ol>
<li><p>文本输出TextOutputFormat<br><strong>默认的输出格式是TextOutputFormat</strong>，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputF ormat调用toString0方法把它们转换为字符串。</p>
</li>
<li><p>SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，<strong>这便是一种好的输出</strong></p>
</li>
</ol>
<p><strong>格式，因为它的格式紧凑，很容易被压缩。</strong><br>3. 自定义OutputFormat<br>    根据用户需求，自定义实现输出。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">job.setOutputFormatClass(FilterOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h2 id="7-Shuffle机制"><a href="#7-Shuffle机制" class="headerlink" title="7. Shuffle机制"></a>7. Shuffle机制</h2><p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615203406.png" alt="1"></p>
<h3 id="7-1-Partition分区"><a href="#7-1-Partition分区" class="headerlink" title="7.1 Partition分区"></a>7.1 Partition分区</h3><p>要求将统计结果按照条件输出到不同文件中(分区)。比如:将统计结果按照手机归属地不同省份输出到不同文件中(分区)</p>
<p>默认Partitioner分区</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartioner</span>&lt;<span class="title">K</span>，<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value，<span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    	<span class="keyword">return</span> (key.hashCode () &amp; Integer.MAX_VALUE) % numReduceTasks ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>默认分区是根据key的<strong>hashCode对ReduceTasks个数取模得到的</strong>。用户没法控制哪个key存储到哪个分区。</p>
<p>自定义Partition后,要根自定Partition的逻讲设置对应数量的ReduceTask。<code>job. setNumRednceTasks(xxx);</code></p>
<p><strong>注意：如果ReduceTask默认1个，则不管多少个分区，都还是产生一个结果文件</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615205548.png" alt="3"></p>
<h3 id="7-2-WritableComparable排序"><a href="#7-2-WritableComparable排序" class="headerlink" title="7.2 WritableComparable排序"></a>7.2 WritableComparable排序</h3><p>排序是MapReduce框架中最重要的操作之一。<br>MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。<strong>而默认排序是按照字典顺序排序</strong>，<strong>且实现该排序的方法是快速排序。</strong></p>
<ul>
<li>排序种类：</li>
</ul>
<p>(1) 部分排序<br>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。</p>
<p>(2) 全排序<br>最终输出结果只有一个文件， 且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在<br>处理大型文件时效率极低，因为一台机器处理所有文件,完全丧失了MapReduce所提供的并行架构。</p>
<p><strong>(3) GroupingComparato分组排序</strong><br><strong>在Reduce端对key进行分组。应用于:在接收的key为bean对象时,想让一个或几个字段相同(全部</strong><br><strong>字段比较不相同)的key 进入到同一个reduce方法时，可以采用分组排序。</strong></p>
<p><strong>(4) 二次排序</strong><br><strong>在自定义排序过程中，如果compareTo中的判断条件为两个即为1二次排序。</strong></p>
<ol>
<li>MapTask排序</li>
</ol>
<p>对于MapTask，它会<strong>将处理的结果暂时放到环形缓冲区中</strong>，当环形缓冲区使用率达到一定阈值80%后，<strong>再对缓冲区中的数据进行一次快速排序，并溢写到磁盘上</strong>，然后会对磁盘上所有文件进行<strong>归并排序。</strong></p>
<ol start="2">
<li>ReduceTask排序</li>
</ol>
<p>对于ReduceTask，它从每个MapTask上远程拷贝copy相应的数据文件</p>
<p>（1）如果<strong>文件大小</strong>超过一定阈值，则溢写磁盘上，否则存储在内存中。<strong>如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件</strong>。</p>
<p>（2）如果<strong>内存中文件大小或者数目</strong>超过一定阈值，则进行一次<strong>合并后combiner将数据溢写到磁盘上</strong>。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据<strong>进行一次归并排序。</strong></p>
<ul>
<li><strong>案例实战</strong></li>
</ul>
<p>增加自定义分区，<strong>分区按照省份手机号设置</strong>，文件中按照<strong>总流量内部降序排序。</strong></p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615204325.png" alt="img"></p>
<p>（1）增加自定义分区类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(FlowBean key, Text value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">   <span class="comment">// 1 获取手机号码前三位</span></span><br><span class="line">   String preNum = value.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">   <span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line">   <span class="comment">// 2 根据手机号归属地设置分区</span></span><br><span class="line">   <span class="keyword">if</span> (<span class="string">"136"</span>.equals(preNum)) &#123;</span><br><span class="line">   partition = <span class="number">0</span>;</span><br><span class="line">   &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(preNum)) &#123;</span><br><span class="line">	partition = <span class="number">1</span>;</span><br><span class="line">   &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(preNum)) &#123;</span><br><span class="line">	partition = <span class="number">2</span>;</span><br><span class="line">   &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(preNum)) &#123;</span><br><span class="line">	partition = <span class="number">3</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> partition;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）在驱动类中添加分区类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 加载自定义分区类</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line"><span class="comment">// 设置Reducetask个数</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<p>（3）排序</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> result;</span><br><span class="line">	<span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">	<span class="keyword">if</span> (sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = -<span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = <span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">		result = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（4）Mapper</p>
<p>就简单的获取然后<strong>拼接&lt;对象，总流量&gt;</strong>就行</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">	FlowBean bean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">	Text v = <span class="keyword">new</span> Text();</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		<span class="comment">// 2 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">		<span class="comment">// 3 封装对象</span></span><br><span class="line">		String phoneNbr = fields[<span class="number">0</span>];</span><br><span class="line">		<span class="keyword">long</span> upFlow = Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">		<span class="keyword">long</span> downFlow = Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">        </span><br><span class="line">		bean.set(upFlow, downFlow);</span><br><span class="line">		v.set(phoneNbr);</span><br><span class="line">		<span class="comment">// 4 输出</span></span><br><span class="line">		context.write(bean, v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（5）Reduce</p>
<p>将<strong>排序和分区</strong>后的数据直接输出就行。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		<span class="comment">// 循环输出，避免总流量相同情况</span></span><br><span class="line">		<span class="keyword">for</span> (Text text : values) &#123;</span><br><span class="line">			context.write(text, key);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="8-Combiner合并"><a href="#8-Combiner合并" class="headerlink" title="8. Combiner合并"></a>8. Combiner合并</h2><p>（1）Combiner和Reducer的区别在于运行的位置：</p>
<p>Combiner是在每一个MapTask所在的节点运行，</p>
<p>Reducer是接收全局所有Mapper的输出结果;</p>
<p>（2）Combiner的意义就是对<strong>每一个MapTask的输出进行局部汇总，以减小网络传输量。</strong></p>
<p>例如：<img src="https://gitee.com/rnang0/blogimage/raw/master/20210615210722.png" alt="4"></p>
<p>（3）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，<strong>Combiner的输出kv应该跟Reducer的输入kv类型要对应起来。</strong></p>
<ul>
<li>实战</li>
</ul>
<p>直接将Reduce作为Combiner合并组件即可，避免再写一个。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line">	IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1 汇总</span></span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span>(IntWritable value :values)&#123;</span><br><span class="line">			sum += value.get();</span><br><span class="line">		&#125;</span><br><span class="line">		v.set(sum);</span><br><span class="line">		<span class="comment">// 2 写出</span></span><br><span class="line">		context.write(key, v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>将WordcountReducer作为Combiner在WordcountDriver驱动类中指定</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 指定需要使用Combiner，以及用哪个类作为Combiner的逻辑</span></span><br><span class="line">job.setCombinerClass(WordcountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615211038.png" alt="5"></p>
<h2 id="9-GroupingComparator分组排序"><a href="#9-GroupingComparator分组排序" class="headerlink" title="9. GroupingComparator分组排序"></a>9. GroupingComparator分组排序</h2><p><strong>对要输入到Reduce阶段的数据</strong>，根据某一个或几个字段进行分组，再输入到Reduce中。</p>
<p>例如：<strong>现在要输出每个订单中最贵的。</strong><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615211202.png" alt="6"></p>
<p>（1）利用“订单id和成交金额”作为key，可以<strong>将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序</strong>，发送到Reduce。</p>
<p>（2）在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后<strong>取第一个</strong>即是该订单中最贵商品。</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615211405.png" alt="7"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//map阶段排序二次排序</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean o)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> result;</span><br><span class="line">		<span class="keyword">if</span> (order_id &gt; o.getOrder_id()) &#123;</span><br><span class="line">			result = <span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (order_id &lt; o.getOrder_id()) &#123;</span><br><span class="line">			result = -<span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">// 价格倒序排序</span></span><br><span class="line">			result = price &gt; o.getPrice() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//分组排序，订单id相同则认为一组</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>(OrderBean<span class="class">.<span class="keyword">class</span>, <span class="title">true</span>)</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">		OrderBean aBean = (OrderBean) a;</span><br><span class="line">		OrderBean bBean = (OrderBean) b;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">int</span> result;</span><br><span class="line">		<span class="keyword">if</span> (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123;</span><br><span class="line">			result = <span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123;</span><br><span class="line">			result = -<span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			result = <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reduce</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		context.write(key, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 8 设置reduce端的分组</span></span><br><span class="line">job.setGroupingComparatorClass(OrderGroupingComparator<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h2 id="10-MapTask与ReduceTask工作机制（-补充）"><a href="#10-MapTask与ReduceTask工作机制（-补充）" class="headerlink" title="10. MapTask与ReduceTask工作机制（ 补充）"></a>10. MapTask与ReduceTask工作机制（ 补充）</h2><p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615212100.png" alt="8"></p>
<p>​    （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p>
<p>​    （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p>
<p>​    （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p>
<p>​    （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>​    溢写阶段详情：</p>
<p>​    步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p>
<p>​    步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>​    步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p>
<p>​    （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>​    当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。</p>
<p>​    在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>​    让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615212211.png" alt="9"></p>
<p>​    （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>​    （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p>
<p>​    （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p>
<p>​    （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>
<p>2．设置ReduceTask并行度（个数）</p>
<p>​    ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 默认值是1，手动设置为4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<h1 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h1><p>Yarn是一个<strong>资源调度平台</strong>，负责为<strong>运算程序提供服务器运算资源</strong>，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的<strong>应用程序。</strong></p>
<h2 id="1-Yarn基本架构"><a href="#1-Yarn基本架构" class="headerlink" title="1. Yarn基本架构"></a>1. Yarn基本架构</h2><p>​    YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成，如图4-23所示。</p>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615220542.png" alt="1"></p>
<h2 id="2-Yarn工作机制（MR作业提交过程）"><a href="#2-Yarn工作机制（MR作业提交过程）" class="headerlink" title="2. Yarn工作机制（MR作业提交过程）"></a>2. Yarn工作机制（MR作业提交过程）</h2><p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615220649.png" alt="2"></p>
<p>​    （1）MR程序提交Job到客户端所在的节点。</p>
<p>​    <strong>（2）YarnRunner向ResourceManager申请一个Application。（一个Job就是一个应用）</strong></p>
<p>​    （3）RM将该应用程序的<strong>资源路径hdfs</strong>返回给YarnRunner。</p>
<p>​    （4）该程序将运行所需资源提交到HDFS上。</p>
<p>​    <strong>（5）程序资源提交完毕后，申请运行mrAppMaster。</strong></p>
<p>​    <strong>（6）RM将用户的请求初始化成一个Task。</strong></p>
<p>​    <strong>（7）其中一个NodeManager领取到Task任务。</strong></p>
<p>​    （8）该NodeManager创建容器Container，并产生MRAppmaster。</p>
<p>​    <strong>（9）Container从HDFS上拷贝资源到本地。</strong></p>
<p>​    <strong>（10）MRAppmaster向RM 申请运行MapTask资源。</strong></p>
<p>​    （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<p>​    （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p>
<p>​    <strong>（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</strong></p>
<p>​    （14）ReduceTask向MapTask获取相应分区的数据。</p>
<p>​    （15）程序运行完毕后，MR会向RM申请注销自己。</p>
<h2 id="3-资源调度器——FIFO、Capacity-Scheduler容量-和Fair-Scheduler公平"><a href="#3-资源调度器——FIFO、Capacity-Scheduler容量-和Fair-Scheduler公平" class="headerlink" title="3. 资源调度器——FIFO、Capacity Scheduler容量 和Fair Scheduler公平"></a>3. 资源调度器——FIFO、Capacity Scheduler容量 和Fair Scheduler公平</h2><p>Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler容量 和Fair Scheduler公平。</p>
<p><strong>Hadoop2.7.2默认的资源调度器是Capacity Scheduler。</strong></p>
<ul>
<li>FIFO</li>
</ul>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615221035.png" alt="1"></p>
<ul>
<li>Capacity Scheduler</li>
</ul>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615221042.png" alt="2"></p>
<ul>
<li>Fair Scheduler</li>
</ul>
<p><img src="https://gitee.com/rnang0/blogimage/raw/master/20210615221030.png" alt="3"></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">rnang0</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://rnang0.github.io/2021/05/08/Hadoop/">http://rnang0.github.io/2021/05/08/Hadoop/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://rnang0.github.io" target="_blank">rnang0 Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/HDFS/">HDFS</a><a class="post-meta__tags" href="/tags/MapReduce/">MapReduce</a></div><div class="post_share"><div class="social-share" data-image="https://gitee.com/rnang0/blogimage/raw/master/20210512182016.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/img/wechat.jpg" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2021/06/09/HBase%E5%92%8CHive/"><img class="prev_cover" src="https://gitee.com/rnang0/blogimage/raw/master/20210625163109.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">HBase数据库、Hive数据仓库</div></div></a></div><div class="next-post pull_right"><a href="/2021/04/29/Go%E7%9A%84Web%E6%A1%86%E6%9E%B6/"><img class="next_cover" src="https://avatars.githubusercontent.com/u/7894478?v=3" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Go的Web框架</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By rnang0</div><div class="footer_custom_text">Hi, welcome to my <a href="http://rnang0.github.io/">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script></body></html>